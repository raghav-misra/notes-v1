# cmsc714; 02.04.2025

## Comparing models

### Shared memory

**Threads** communicate within a process.

Easy to get a shared memory program working...

- Can be hard to debug as all processors have access to all data.
- Need synchronization to ensure multiple processors aren't modifying same piece of data.

### Distributed memory

**Processes** communicate.

- Doesn't mean that each process can't be multi-threaded.

Subset of data assigned to different processors.

- Assign a "cube" to each process: maximize volume to surface ratio.
- Minimizes communication/computation ratio.
- <Block, Block, Block> distribution.

Message passing between processors.

- Use send/receive to move the data.
- No need for barriers, send/receive provides synchronization.
- We want sends as early as possible and receives as *late* as possible so that the communication time isn't blocking work.

Easier to debug? Maybe.

- Forces you to explicitly consider data locality. (sends/receives)
- Better performance and scaling.
- Harder to get the code running in the first place.

## More applications

### Databases

Too much data to fit in memory/disk!

- Data mining applications.
- Imaging applications. (Forklift to load tapes by the pallet.)

Sources of parallelism:

- Within a large transaction.
- Among multiple transactions.

`JOIN` operation:

- Form single table from two, based on a common field.

### Parallel search (TSP)

It may appear to be a faster than expected speedup, but you may have just gotten lucky (one thread finds the target significantly faster than others).

Algorithm:

- Compute path on a processor
  - If our path is shorter, send it to the others.
  - Stop searching a path when it is longer than the shorter.

## Measuring efficacy

### Terms

**Load balance:** Try to balance amount of work assigned to different threads and processes.

**Grain size:** ratio of computation-to-communication.

### Ensuring fair speedup

$T_{\text{serial}} =$ fastest of:

- The *fastest serial algorithm*.
- Parallel algorithm executed on one process/thread.

If speedup appears to be super-linear:

- Check for memory hierarchy effects: might be due to increased cache and memory sizes.
- Verify order of operations is the same in parallel and serial cases.



## OpenMP!

### Overview

Provides support for *shared-memory parallelism*.

- Simple, portable model.
- Allows for both shared and private data.
- Provided parallel `do` loops. (Huge!)

It includes:

- Automatic support for fork/join parallelism.
- *Reduction variables:* accumulation across parallel loop iterations.
- *Atomic statements:* one process executes it at a time.

Both thread-local and shared memory (depending on how directives are used).

- Parallelism: directives for parallel loops and functions.
- The *compiler converts programs to be multithreaded*. (Uses `pthreads` internally.)
  - Not you! You still need to write a correct, parallelizable program.
- Not able to run on more than one node in a cluster.

Example:

```cpp
#pragma omp parallel for private(i)
for (i = 0; i < NUPDATE; i++) {
    int ran = random();
    table[ ran & (TABSIZE - 1) ] ^= stable[ ran >> (64 - LSTSIZE) ];
}
```

- `#pragma omp parallel for private(i)` is the preprocessing directive that OpenMP knows to look for and handle.
  - Divide up loop iterations across however many threads.
  - Need to know number of iterations *before* the loop is executed.
  - This is how the compiler can optimize this to be parallelized.

### More characteristics

Yippee!

- Not a full parallel language; it's a language extension.
- Set of compiler directives and library routines.
- Used to create parallel C, C++, Fortran programs.
- Usually for parallelizing loops.
- Standardize ~25 years of SMP practice.

Implementation:

- C/C++ compiler directives using `#pragma omp <directive>`.
- Parallelism can be specified for regions and loops.
- Data can be either:
  - *Private*, each thread keeps its own copy.
  - *Shared*, single copy across all threads.

Fork/join parallelism (restricted form of MIMD):

- Normally a single control thread. (Primary)
- Worker threads are spawned when a parallel region/loop is encountered.
- Barrier synchronization is required at the end of a parallel region.

*OpenMP philosophy: there is a default way to do things; if you need more control, you opt into it.*

### Example of task-level parallelism

```cpp
double a[1000];
omp_set_num_threads(4);
#pragma omp parallel
{
    int id = omp_thread_num();
    food(id, a);
}
printf("all done \n");
```

- The pragma applies to the next statement, which is the block.
- So for each of the four threads, the following are called in parallel:
  - `foo(0, a);`
  - `foo(1, a);`
  - `foo(2, a);`
  - `foo(3, a);`
- There is an implicit barrier at the close of the block. All threads are waited for, before "all done" is printed.



### Example of loop-level parallelism

Parallelizing a `for` loop:

```cpp
#pragma omp parallel for
for (i = 0; i < N; i++) {
    foo(i);
}
```

This is shorthand for something like:

```cpp
#pragma omp parallel
{
    int id, i, nthreads, start, end;
    id = omp_get_thread_num();
    nthreads = omp_get_num_threads;
    start = id * N / nthreads;
    end = (id + 1) * N / nthreads;
    for (i = start; i < end; i++) {
        foo();
    }
}
```

Notice that each thread will take a chunk of the iterations. So like four iterations on two threads, each thread might take two of the iterations.

### Race conditions

**Race condition:** program outcome depends on scheduling order of the threads; unintended consequences of sharing variables.

How can we prevent data races?

- Use synchronization? Locks.
- Change how data is stored. That is, shared to private.

### Details on writing OpenMP

**Pragma:** compiler directive in C or C++.

```cpp
#pragma omp construct [clause [clause] ...]
```

**Hello world program:**

```cpp
#include <stdio.h>
#include <omp.h>

int main(void) {
    #pragma omp parallel
    printf("Hello, world.\n");
    return 0;
}
```

- To compile this: `gcc -fopenmp hello.c -o hello`.
- Setting number of threads: `export OMP_NUM_THREADS=2`.
  - Environment variable.
  - Notice we didn't explicitly set in the program, # of threads.
    - So this is how we define that.

**General form of parallel `for` region:**

```cpp
#pragma omp parallel for [clause [clause] ...]
```

- Some form of a counting loop.
- We need to be able to determine the number of iterations before the loop executes. *Not at compile time!*

```cpp
int main(int argc, char **argv) {
    // ...
    #pragma omp parallel for
    for (int i = 0; i < n; i++) {
        z[i] = a * x[i] * y[i];
    }
    // ...
}
```

- All you need to parallelize is the pragma!

**Ways to set number of threads:**

- Environment variable: `export OMP_NUM_THREADS=X`
- Use `omp_set_num_threads(int num_threads)`
- `int omp_get_num_procs(void)` returns the number of available processors/cores.
  - Can be used to decide the number of threads to create.
  - Would take a call to `omp_set_num_threads` after.

**Data sharing defaults:**

- Most variables shared by default.
- Globals are shared.
- *Exception:* loop index variables are private by default.
- Stack variables in function calls from parallel regions are *thread-private*.

