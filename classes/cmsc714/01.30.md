# cmsc714; 01.30.2025

## Last class... (I wasn't there)

**Why parallel computing?** Speed and cost

Parallel computing basics

- Processing elements, memory, network, disks
- SIMD, MIMD, SPMD, dataflow
- Networks
  - Bus, ring, tree, mesh (2D or 3D), hypercube.
- Memory
  - Latency and throughput (bandwidth)
  - Shared vs. distributed (physically and logically)
  - UMA vs. NUMA

## Memory systems

### Key performance issues

**Latency:** time to retrieve first byte.

- DRAM (dynamic RAM) might have 100ns.
- Disk might have 1ms.

**Throughput:** Average bytes/second.

- Play with parallelism.
- How many bytes moving at the same time?

Depending on constraints, both can matter. Throughput is consistently increasable; latency would have physical limits.

### Design issues

Where is the memory?

- Divided among each node.
- Centrally located on communication network: remote from all nodes.
  - Flash memory can be shared as storage across network; faster than disk access.

Access by processors:

- Can all processors can get to all the memory?
- Is the access time uniform?
  - UMA (uniform memory access).
  - NUMA (non-uniform memory access).

## Coordination

Parallelism, in our view, is processors working *together* to solve a problem.

**Synchronization:**

- Protection of a single object/data (e.g., locks)
- Coordination of processors (e.g., barriers)
  - Barrier: no processor passes until all processors get there.

Single problem / many processors $\rightarrow$ **how to assign work?**

- Load balance: processors have equal work.
  - Say, $n$ work and $p$ processors. We only care about total time.
  - Work is done when the *last processor* finishes its work.
  - Optimize this by evenly assigning work to each processor.
- Overhead from coordination and synchronization.
  - Only exists in parallelism and we want to minimize it.
- Not uncommon that optimizing load balancing increases the overhead.
  - For example, idle time when processor $1$ waits at a barrier for processor $2$ to get there.

## Serial vs. parallel code

### Terminology

A **thread** is a unit of execution managed by the OS.

- Threads can share memory, multiple ones can run in the same *address space*. Because they belong to the same process (see below).

A **process** is an address space with one or more threads running in it.

- Multiple processes allow for multiple nodes, as long as they can communicate.

Serial vs. parallel:

- **Serial (sequential) code** can *only run in a single thread in a single process*.
- **Parallel code** can be run on *one or more threads in one or many processes*.

### Sources of parallelism

Statements:

- Called "control parallel"
- Can perform a series of steps in parallelism
- Basis of dataflow computers

Loops:

- Called "data parallel"
- Most common source of parallelism
- *Example:* running some independent operation on every element of data in a dataset.
- Each processor/core gets one (or more) iterations of this loop to perform.
- You can achieve large-scale parallelism. *The dream!*

### Examples of parallelism

*Embarrassingly parallel:* multiple independent jobs (i.e., different simulations)

*Scientific applications:*

- Dense linear algebra (divide up/partition the matrix)
- Physical system simulations (divide physical space)

*Databases:*

- Biggest success of parallel of computing (divide tuples)
- Exploits the semantics of *relational algebra*.
  - Querying: order may not matter!

*Artificial intelligence:*

- Search problems (divide search space)
- Pattern recognition and image processing (divide the image)

### Metrics in application performance

$\log (\text{\# cores})$ vs. $\log(\text{exec. time})$ is a negative linear relationship.

**Speedup:** ratio of time on one node to the time on $n$ nodes.

- Not changing the same of the problem; strong scaling.
- We want *linear speedup*, i.e., double the cores should half the time.
  - Not perfect; overhead from synchronization and coordination.

*Note:* the best parallel algorithm for your work is not necessarily the best serial algorithm for your work.

- Speedup ideally compares the best serial (single-core) algorithm with the best parallel algorithm; not always the same.

*Super-linear speedup* is possible due to adding more memory/cache.

- For search problems, maybe one specific core found the target in less time than the serial algorithm in total would.

*Iso-speedup* scales the data size up with the number of nodes.

- Here, our goal is a flat horizontal curve.
- **Why do we care?** Often the goal isn't to run the same problem faster, but to get a solution for more data.

**Amdahl's Law:** Strong scaling is limited by the parts of your code you can't speed up. That is, the part that *has* to run sequentially.
$$
\text{max speedup}=\frac{1}{1 - f + \frac{f}{s}}, \text{ as } s \rightarrow \infty,
$$
where $f$ is the percent of the program which is *parallelizable*.

**Computation to communication ratio:** we want to maximize this.

- Communication is basically $100\%$ overhead.

### How to write parallel programs?

Use old serial code:

- Compiler converts it to parallel.
- Called the "dusty deck problem."

Serial language + communication library:

- No compiler changes needed.
- MPI uses this approach.

New language for parallel computing:

- Requires all code to be re-written.
- Hard to create a language that provides high performance on different platforms.

Hybrid approach: old language, new constructs.

- PGAS (Partitioned Global Address Space): variants of C/Fortran/Java with combination of shared memory view of data, but awareness of data being distributed on the hardware.
- Have parallel loops and synchronization operations.