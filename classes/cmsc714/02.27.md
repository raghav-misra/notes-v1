# cmsc714; 02.27.2025

## Background on GPUs

### Accelerators

Original IBM cell processors.

- Used in PS3.

GPUs: NVIDIA, AMD, Intel

- First programmable GPU: NVIDIA GeForce 256.
- Around 1999-2001, early GPGPU results (scientific computing).

In 2013, NAMD: used for molecular dynamics simulations on a supercomputer.

- 3000 NVIDIA Tesla GPUs

### GPGPU Hardware

Difference from CPUs:

- CPUs have a "few" cores. (Zaratan has 128 cores.)
  - Cache hierarchy. L1 closest to the core. Then L2 then L3.
  - Cores are very fast but few.
- GPUs have many more cores. (NVIDIA A100 has 17,212 cores.)
  - A100 has 0.76 GHz frequency whereas CPUs have higher: i9 11900K has 3.3 GHz.
  - Way more cores meaning you can divide work across a large number of threads for simultaneous.

Most instructions are also executable on the GPU, but you need to think about the best way to *exploit* the GPU.

- Why can't you just put faster cores together like in a GPU? Physical problem due to heat generation! Also, lots of high-frequency cores will draw a lot of power.

NVIDIA H100 chip: made up of lots of streaming multiprocessors (SMs).

- Each SM is insanely complex. 
- 4 tensor cores (targeted to DL matrix computation).
- 64 FP64 cores for large floating-point computations.
- 128 FP32 cores for small floating-point computations.
- 64 INT32 cores for integer computations.

Each H100 has 144 such SMs.

## Connecting to GPUs

### Nodes with GPUs

We use NIC (network interface card) to connect the node to the network.

- Node here referring to the singular machine with CPU, memory, optionally GPU.

We use NVLink for high-speed data transfer between GPUs. And we use PCIe to transfer data between CPU and GPU.

## CUDA

### What is it

**CUDA** is a programming model for NVIDIA GPUs.

- Allows developers to use C++ to program for GPUs.
- Built around threads, blocks, grids.
- Terminology:
  - Host: CPU.
  - Device: GPU.
  - CUDA kernel: a function which gets executed on GPU.

Recall OpenMP: `#pragma openmp` and everything is just done for you.

- CUDA is different and more manual.
- Common programming model with CUDA is SIMD.

Recall a **thread**, a single unit of execution.

- In CUDA we have **blocks**, which are collections of threads.
  - There is $\leq 1024$ threads in a given block.
  - Handled by CUDA, but blocks are mapped to SMs. (Streaming multiprocessors as defined above.)
- Also, we have **grids**, which are groups of blocks.

### Writing a CUDA kernel

Three steps:

1. Explicitly copy data from the CPU to the GPU.
2. Load the GPU program (kernel) and execute it.
3. Copy the results back to the host (CPU) memory.

Example program:

```cpp
double *d_Matrix, *h_Matrix;
h_Matrix = new double[N];

cudaMalloc(&d_Matrix, sizeof(double) * N);

// ... initialize h_Matrix somehow
cudaMemcpy(d_Matrix, h_Matrix, sizeof(double) * N, cudaMemcpyHostToDevice);

// ... some GPU computations

cudaMemcpy(h_Matrix, d_Matrix, sizeof(double) * N, cudaMemcpyDeviceToHost);

```

Example CUDA function:

```cpp
__global__ void saxpy(float *x, float *y, float alpha) {
    int i = threadIdx.x;
    y[i] = alpha * x[i] + y[i];
}

int main() {
    // assume x, y, alpha are copied on GPU already
    saxpy<<1, N>>(x, y, alpha);
    // 1 block; N threads per block
}
```

When you have more than $1024$, you need multiple blocks!

Then the math for `int i` would use both the block number and the thread number: `int i = blockDim.x * blockIdx.x + threadIdx.x`. You also need to check to make sure you aren't doing unneeded math with an extra thread.

```c
__global__ void saxpy(float *x, float *y, float alpha, int N) {
    int i =  blockDim.x * blockIdx.x + threadIdx.x;
    if (i < N) {
    	y[i] = alpha * x[i] + y[i];
    }
}

int main() {
    int threadsPerBlock = 512;
    int numBlocks = N / threadPerBlock
        			+ (N % threadsPerBlock != 0);
    saxpy<<1, N>>(numBlocks, threadsPerBlock, alpha);

}
```



### Compiling and Running

Compiling, need to use NVIDIA's compiler: `nvcc`. Then you can run output as an executable.

You can combine MPI with CUDA, as you can do MPI with OpenMP.

