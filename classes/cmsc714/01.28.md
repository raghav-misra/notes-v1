# cmsc714; 01.28.2025

## Overview stuff

I wasn't here (not yet enrolled in class) this day, I'm just backreading the slides and googling what I don't understand.

### Course structure

Class is an **introduction to parallel computing**.

- Seminar style: on history and recent advancements.
- Programming models, hardware, applications, compilers, system software, and tools.

Work required:

- Two small coding assignments: MPI and OpenMP.
  - **MPI (Message Passing Interface):** standardized message passing system which allows processes to communicate amongst each other.
  - **OpenMP** is an API designed for parallelism in shared-memory systems. Primarily used for multithreading within a process, as opposed to cross-process communication.
- Midterm exam.
- Class participation: prep questions based on readings.
- Group project (3 students per group).

### Course topics

With estimated time spent:

- *Intro to parallel computing*, **1 week**
- *Programming models*, **3 weeks**
- *Parallel architectures and networks*, **3 weeks**
- *Debugging and instrumentation*, **1 week**
- *Performance tools*, **2 weeks**
- *OS, runtime systems, parallel I/O*, **2 weeks**
- *Commercial and scientific applications*, **2 weeks**

## What is parallel computing?

### Definition and motivations

Parallel computing requires:

- More than one processing element/core.
- Nodes (with multiple cores) connected to a communication network.
- Nodes working together to solve a single problem.
  - Sometimes a single node is enough

**Speed**

- Need to get results faster than is possible with sequential/serial.
  - A late weather forecast is useless.
- Could come from:
  - More processing elements.
  - More memory/cache.
  - More disks/secondary storage.
- Example: speeding up scientific simulations.

**Cost**: cheaper to buy many smaller machines.

### Parallel computing architecture

What does a parallel computer look like?

- **Hardware:** processors, communication, memory, coordination.
- **Software:** programming model, communication libraries, OS.

**Processing element (PE):** computational unit that can do work independently.

- We use multiple PEs to parallelize our work.
- Key decisions: How many? How powerful? Custom, or off-the-shelf?

Major styles of parallel computing:

- **SIMD (single instruction, multiple data):**
  - A single instruction is executed simultaneously on multiple data elements.
  - *Examples:* image processing, matrix operations, deep learning.
- **MIMD (multiple instruction, multiple data):**
  - Different PEs will execute different instructions on different data elements.
  - *Example:* a multi-core CPU running different programs on each of its cores.
- **SPMD (single program, multiple data):**
  - A special case of MIMD where all processing elements run the *same program*, but operate on different data.
- **Dataflow:** computation is driven by data *availability* rather than sequential instruction execution. 
  - I am not going to look into this more deeply for now since it doesn't really make sense to me LOL and hasn't been covered later yet.



